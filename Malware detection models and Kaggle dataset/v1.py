# import pandas as pd
# from torch import nn
# from sklearn.model_selection import train_test_split
# import torch.optim as optim
# from sklearn.preprocessing import MinMaxScaler
# from torch.utils.data import DataLoader,TensorDataset
# import torch.nn.functional as F
# import torch
# SAMPLE_SIZE=1079
# ITERATIONS=40
# device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
# df=pd.read_csv('/home/virtual/Desktop/MLforEE/projekt_maldetect/data/dynamic_api_call_sequence_per_malware_100_0_306.csv')
# mal_df=df[df['malware']==1]
# ben_df=df[df['malware']==0]

# X=df.iloc[:,1:101].values
# y=df.iloc[:,-1].values

# class SimpleMLP(nn.Module):
#     def __init__(self, input_size=100):
#         super(SimpleMLP, self).__init__()
#         # Hidden layers
#         self.fc1 = nn.Linear(input_size, 64)  # first hidden layer
#         self.fc2 = nn.Linear(64, 32)          # second hidden layer
#         # Output layer
#         self.fc3 = nn.Linear(32, 1)           # binary output
        
#     def forward(self, x):
#         x = F.relu(self.fc1(x))
#         x = F.relu(self.fc2(x))
#         x = self.fc3(x)  # raw logits
#         return x
# model=SimpleMLP()
# model.to(device)
# Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3)

# Xt=torch.tensor(Xtrain)
# Xv=torch.tensor(Xtest)
# yt=torch.tensor(ytrain)
# yv=torch.tensor(ytest)

# traindataset=TensorDataset(Xt,yt)
# testdataset=TensorDataset(Xv,yv)
# criterion=nn.BCEWithLogitsLoss()
# optimizer=torch.optim.Adam(model.parameters(),lr=1e-3)

# train_loader = DataLoader(traindataset, batch_size=164, shuffle=True)
# test_loader = DataLoader(testdataset, batch_size=164, shuffle=False)

# num_epochs = 100

# for epoch in range(num_epochs):
#     model.train()  # Set model to training mode
#     running_loss = 0.0

#     for inputs, labels in train_loader:
#         inputs = inputs.float().to(device)    
#         labels = labels.float().to(device)     
    
#         optimizer.zero_grad()
#         outputs = model(inputs).squeeze()
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()
    
#     print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")


# model.eval()  # set evaluation mode (disables dropout, etc.)
# correct = 0
# total = 0

# # with torch.no_grad():  # no gradients needed
# #     for inputs, labels in test_loader:
# #         outputs = model(inputs).squeeze()
# #         predictions = torch.sigmoid(outputs) > 0.5  # convert logits â†’ binary
# #         correct += (predictions == labels).sum().item()
# #         total += labels.size(0)

# # print(f"Accuracy: {100 * correct / total:.2f}%")


# with torch.no_grad():     # Disable gradient computation
#     for inputs, labels in test_loader:
#         # âœ… Move data to same device as model
#         inputs = inputs.float().to(device)
#         labels = labels.float().to(device)

#         # Forward pass
#         outputs = model(inputs).squeeze()

#         # Convert logits â†’ binary predictions
#         predictions = torch.sigmoid(outputs) > 0.5

#         # Compute accuracy
#         correct += (predictions == labels).sum().item()
#         total += labels.size(0)

# print(f"âœ… Accuracy: {100 * correct / total:.2f}%")


# torch.save(model,"FeedForwardNN.pth")

# # for epoch in range(num_epochs):
# #     model.train()  # Set model to training mode
# #     running_loss = 0.0

# #     for batch_idx, (inputs, labels) in enumerate(train_loader):
# #         inputs = inputs.float().to(device)
# #         labels = labels.float().to(device)

# #         optimizer.zero_grad()
# #         outputs = model(inputs).squeeze()
# #         loss = criterion(outputs, labels)
# #         loss.backward()
# #         optimizer.step()
# #         running_loss += loss.item()

# #         # ðŸ” Show sample data for the first batch of each epoch
# #         if batch_idx == 0:
# #             print(f"\nðŸŸ© Epoch {epoch+1}/{num_epochs} â€” Training Sample:")
# #             print("Input (first 2 samples):")
# #             print(inputs[:2].detach().cpu().numpy())       # view first 2 input vectors
# #             print("Labels:", labels[:2].detach().cpu().numpy())
# #             print("Raw outputs (logits):", outputs[:2].detach().cpu().numpy())
# #             print("Predicted probabilities:", torch.sigmoid(outputs[:2]).detach().cpu().numpy())

# #     avg_loss = running_loss / len(train_loader)
# #     print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

# #     # ðŸ§ª Evaluate on test data at end of each epoch
# #     model.eval()
# #     with torch.no_grad():
# #         test_inputs, test_labels = next(iter(test_loader))  # just one batch for quick check
# #         test_inputs = test_inputs.float().to(device)
# #         test_labels = test_labels.float().to(device)
# #         test_outputs = model(test_inputs).squeeze()
# #         test_preds = torch.sigmoid(test_outputs) > 0.5

# #         print("\nðŸŸ¦ Test Sample:")
# #         print("Test input (first 2 samples):")
# #         print(test_inputs[:2].detach().cpu().numpy())
# #         print("True labels:", test_labels[:2].detach().cpu().numpy())
# #         print("Predicted outputs:", test_preds[:2].detach().cpu().numpy())
import pandas as pd
from torch import nn
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F

# --- Parameters ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Load Data ---
df = pd.read_csv('/home/virtual/Desktop/MLforEE/projekt_maldetect/data/dynamic_api_call_sequence_per_malware_100_0_306.csv')

X = df.iloc[:, 1:101].values   # 100 features
y = df.iloc[:, -1].values      # labels (0 or 1)

# --- Split Dataset ---
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Convert to PyTorch tensors ---
Xt = torch.tensor(Xtrain).float()
Xv = torch.tensor(Xtest).float()
yt = torch.tensor(ytrain).float()
yv = torch.tensor(ytest).float()

# --- Datasets & Dataloaders ---
train_dataset = TensorDataset(Xt, yt)
test_dataset = TensorDataset(Xv, yv)

train_loader = DataLoader(train_dataset, batch_size=164, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=164, shuffle=False)

# --- Define Feedforward Model ---
class SimpleMLP(nn.Module):
    def __init__(self, input_size=100):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# --- Initialize Model ---
model = SimpleMLP().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# --- Training Loop ---
num_epochs = 100
best_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    
    # ---- Training ----
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs).squeeze()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)

    # ---- Validation ----
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs).squeeze()
            preds = torch.sigmoid(outputs) > 0.5
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    accuracy = 100 * correct / total

    # Save best model
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        torch.save(model.state_dict(), "FeedForwardNN.pth")

    print(f"Epoch [{epoch+1}/{num_epochs}] | Loss: {avg_loss:.4f} | Val Accuracy: {accuracy:.2f}%")

print(f"\nâœ… Training complete! Best validation accuracy: {best_accuracy:.2f}%")
torch.save(model.state_dict(), "FeedForwardNN_final.pth")
