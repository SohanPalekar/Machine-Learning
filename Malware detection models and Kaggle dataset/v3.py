import pandas as pd
from torch import nn
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F

# --- Parameters ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Load Data ---
df = pd.read_csv('/home/virtual/Desktop/MLforEE/projekt_maldetect/data/dynamic_api_call_sequence_per_malware_100_0_306.csv')

X = df.iloc[:, 1:101].values   # 100 API-call features
y = df.iloc[:, -1].values      # malware labels (0 or 1)

# --- Train/Test Split ---
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Convert to Tensors ---
Xt = torch.tensor(Xtrain).float().unsqueeze(-1)  # (N, 100, 1)
Xv = torch.tensor(Xtest).float().unsqueeze(-1)
yt = torch.tensor(ytrain).float()
yv = torch.tensor(ytest).float()

traindataset = TensorDataset(Xt, yt)
testdataset = TensorDataset(Xv, yv)

train_loader = DataLoader(traindataset, batch_size=164, shuffle=True)
test_loader = DataLoader(testdataset, batch_size=164, shuffle=False)

# --- Define LSTM Model ---
class SimpleLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super(SimpleLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # Initialize hidden + cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward pass
        out, _ = self.lstm(x, (h0, c0))
        out = out[:, -1, :]  # take last time-step
        out = self.fc(out)
        return out

# --- Initialize Model ---
model = SimpleLSTM(input_size=1, hidden_size=100, num_layers=1).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# --- Training Loop ---
num_epochs = 83
best_accuracy=0.0
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs).squeeze()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")
    avg_loss = running_loss / len(train_loader)
# --- Evaluation ---
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs).squeeze()
            predictions = torch.sigmoid(outputs) > 0.5
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
    accuracy=100*correct/total
    print(f"Epoch [{epoch+1}/{num_epochs}] | Loss: {avg_loss:.4f} | Val Accuracy: {accuracy:.2f}%")   
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        #torch.save(model.state_dict(), "BestLSTM.pth") 
print(f"âœ… LSTM Accuracy: {100 * correct / total:.2f}%")

# --- Save Model ---
#torch.save(model.state_dict(), "LSTM_Classifier.pth")